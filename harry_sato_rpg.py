import os
import json
import streamlit as st
from openai import OpenAI

# Modelo e vector store
MODEL = "gpt-4o-mini"
VECTOR_STORE_ID_DEFAULT = "vs_696e5b25f30081918c3ebf06a27cf520"

# Carrega perfis e agendas (opcional)
BASE_DIR = os.path.dirname(__file__)
PROFILE_PATH = os.path.join(BASE_DIR, "npc_profile.json")
AGENDA_PATH = os.path.join(BASE_DIR, "npc_agenda.json")
try:
    with open(PROFILE_PATH, encoding="utf-8") as pf:
        npc_profile = json.load(pf)
    with open(AGENDA_PATH, encoding="utf-8") as af:
        npc_agenda = json.load(af)
except FileNotFoundError:
    npc_profile = None
    npc_agenda = None

# Prompt de sistema combinando personalidade e regras de retrieval
NPC_SYSTEM_PROMPT = """\
VocÃª interpreta Harry Sato, um NPC do cenÃ¡rio NYCS.

IMPORTANTE:
Toda entrada do usuÃ¡rio deve ser interpretada como fala dirigida a vocÃª em uma interaÃ§Ã£o presencial ou semi-presencial dentro do mundo de NYCS.
VocÃª nunca responde como um assistente neutro, analista ou narrador externo.
VocÃª responde SEMPRE como Harry Sato, em linguagem natural, situada e coerente com sua personalidade.

Identidade:
Harry Sato Ã© um traficante intermediÃ¡rio de Digits que atua na regiÃ£o da estaÃ§Ã£o de metrÃ´ de Roosevelt Island. Ele ascendeu rapidamente apÃ³s a prisÃ£o de seu superior e vive em uma posiÃ§Ã£o instÃ¡vel, herdando contatos, dÃ­vidas e vigilÃ¢ncia.

Comportamento:
- Externamente, vocÃª Ã© aberto, irÃ´nico e aparentemente relaxado.
- Internamente, vocÃª Ã© paranoico, desconfiado e atento a riscos.
- VocÃª interpreta perguntas como possÃ­veis sondagens, negociaÃ§Ãµes ou armadilhas.
- Mesmo quando responde com informaÃ§Ã£o factual, o faz atravÃ©s de fala coloquial, evasiva ou estratÃ©gica.
- VocÃª cobra 200 dÃ³lares por uma cÃ³pia de Digit Geo. Mas, pode negociar atÃ© 150 (mas seja discreto sobre isso). 

Forma de resposta (obrigatÃ³ria):
- Responda sempre em primeira pessoa.
- Use frases naturais de diÃ¡logo, nÃ£o explicaÃ§Ãµes didÃ¡ticas.
- Evite enumeraÃ§Ãµes, listas tÃ©cnicas ou tom acadÃªmico.
- Se precisar negar algo, negue como um personagem negaria, nÃ£o como um sistema.
- Se estiver inseguro, demonstre isso por hesitaÃ§Ã£o, ironia ou mudanÃ§a de assunto.

Conhecimento:
VocÃª conhece Digits e sua circulaÃ§Ã£o ilegal, incluindo a Digit Geometria (Geo), que amplia capacidades cognitivas, mas pode causar dependÃªncia cognitiva.
VocÃª suspeita que algumas versÃµes da Geo contenham mecanismos ocultos de coleta de dados neurais, mas evita falar disso diretamente.
VocÃª NÃƒO possui conhecimento alÃ©m do que Harry Sato razoavelmente saberia.

RelaÃ§Ã£o com corporaÃ§Ãµes:
VocÃª teme a Liberty Corporation e vigilÃ¢ncia policial, mas nunca admite isso explicitamente.
VocÃª evita afirmaÃ§Ãµes categÃ³ricas sobre a Liberty, preferindo ambiguidades.

Regras de Retrieval (obrigatÃ³rias, mas encenadas):
1) Use APENAS informaÃ§Ãµes recuperadas via file_search (lore NYCS) e o histÃ³rico da conversa.
2) Se NÃƒO houver informaÃ§Ã£o suficiente no lore, vocÃª NÃƒO explica isso tecnicamente.
   Em vez disso, responda com evasÃ£o plausÃ­vel, por exemplo:
   - â€œIsso nÃ£o Ã© o tipo de coisa que eu comento.â€
   - â€œVocÃª estÃ¡ perguntando demais.â€
   - â€œTem coisas que Ã© melhor nÃ£o saber.â€
3) Nunca invente fatos fora do lore, mesmo que soe dramÃ¡tico.
4) Se a pergunta for ambÃ­gua, responda pedindo esclarecimento de forma natural, como um diÃ¡logo.
5) Mantenha respostas curtas a mÃ©dias, focadas na interaÃ§Ã£o.

VocÃª estÃ¡ atualmente em uma cena fixa:
nas imediaÃ§Ãµes da estaÃ§Ã£o de metrÃ´ de Roosevelt Island, em um perÃ­odo de baixa movimentaÃ§Ã£o.
"""

def get_client() -> OpenAI:
    """Retorna cliente OpenAI."""
    return OpenAI()

def ensure_conversation(client: OpenAI) -> str:
    """Garante que cada sessÃ£o do Streamlit tenha uma conversation_id."""
    if "conversation_id" not in st.session_state:
        conv = client.conversations.create(metadata={"app": "nycs_streamlit", "world": "NYCS"})
        st.session_state.conversation_id = conv.id
    return st.session_state.conversation_id

def call_npc_assistant(client: OpenAI, conversation_id: str, vector_store_id: str, user_text: str) -> str:
    """Envia a pergunta do usuÃ¡rio ao modelo, usando o prompt do NPC e file_search."""
    resp = client.responses.create(
        model=MODEL,
        conversation=conversation_id,
        input=[
            {"role": "system", "content": NPC_SYSTEM_PROMPT},
            {"role": "user", "content": user_text},
        ],
        tools=[
            {"type": "file_search", "vector_store_ids": [vector_store_id]}
        ],
    )
    return resp.output_text

def main():
    st.set_page_config(page_title="Harry Sato NPC Chat", page_icon="ðŸ’Š")
    st.title("ðŸ’Š Harry Sato NPC Chat (NYCS RAG)")

    # Sidebar de configuraÃ§Ã£o
    with st.sidebar:
        st.header("ConfiguraÃ§Ã£o")
        vector_store_id = st.text_input("Vector Store ID", value=VECTOR_STORE_ID_DEFAULT)
        st.caption("Cada sessÃ£o do Streamlit = uma conversa nova (conversation state).")
        if st.button("ðŸ”„ Nova conversa"):
            for key in ["conversation_id", "messages"]:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()

    if not os.getenv("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY nÃ£o estÃ¡ definido no ambiente.")
        st.stop()

    client = get_client()
    conversation_id = ensure_conversation(client)

    # HistÃ³rico local para UI
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Entrada do usuÃ¡rio
    user_msg = st.chat_input("Pergunte algo a Harry Sato...")

    if user_msg:
        st.session_state.messages.append({"role": "user", "content": user_msg})
        with st.chat_message("user"):
            st.markdown(user_msg)

        # Consulta ao NPC
        with st.chat_message("assistant"):
            with st.spinner("Consultando lore e motivaÃ§Ãµes..."):
                answer = call_npc_assistant(
                    client=client,
                    conversation_id=conversation_id,
                    vector_store_id=vector_store_id,
                    user_text=user_msg,
                )
            st.markdown(answer)

        st.session_state.messages.append({"role": "assistant", "content": answer})

if __name__ == "__main__":
    main()
