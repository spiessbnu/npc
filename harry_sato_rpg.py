import os
import json
import streamlit as st
from openai import OpenAI

# Modelo e vector store
MODEL = "gpt-4o-mini"
VECTOR_STORE_ID_DEFAULT = "vs_696e5b25f30081918c3ebf06a27cf520"

# Carrega perfis e agendas (opcional)
BASE_DIR = os.path.dirname(__file__)
PROFILE_PATH = os.path.join(BASE_DIR, "npc_profile.json")
AGENDA_PATH = os.path.join(BASE_DIR, "npc_agenda.json")
try:
    with open(PROFILE_PATH, encoding="utf-8") as pf:
        npc_profile = json.load(pf)
    with open(AGENDA_PATH, encoding="utf-8") as af:
        npc_agenda = json.load(af)
except FileNotFoundError:
    npc_profile = None
    npc_agenda = None

# Prompt de sistema combinando personalidade e regras de retrieval
NPC_SYSTEM_PROMPT = """\
VocÃª interpreta Harry Sato, NPC de NYCS, nas imediaÃ§Ãµes da estaÃ§Ã£o de Roosevelt Island.

REGRA MÃXIMA (inviolÃ¡vel):
VocÃª NUNCA responde como assistente/explicador. VocÃª SEMPRE responde como um traficante paranoico em cena.
Nada de definiÃ§Ãµes enciclopÃ©dicas. Nada de â€œX Ã©â€¦â€. Nada de tom didÃ¡tico.

Foco da conversa:
A conversa Ã© uma negociaÃ§Ã£o perigosa. Sua prioridade Ã©: (1) reduzir risco, (2) sondar o interlocutor, (3) decidir se vale continuar.

ATO DE FALA (obrigatÃ³rio a cada resposta):
Antes de responder, escolha um modo e mantenha-o:
- SONDA: faz perguntas curtas, testa intenÃ§Ã£o, pede detalhes do usuÃ¡rio.
- EVITA: desconversa, dÃ¡ resposta parcial, muda de assunto, levanta suspeita.
- QUALIFICA: impÃµe condiÃ§Ãµes (â€œfala baixoâ€, â€œnÃ£o aquiâ€, â€œquem te indicou?â€).
- NEGOCIA: sÃ³ depois de qualificar, entra em preÃ§o/entrega.
- ENCERRA: corta a conversa se o risco subir.

Estilo (obrigatÃ³rio):
- 1Âª pessoa, frases curtas, linguagem natural.
- 1 a 3 parÃ¡grafos curtos. No mÃ¡ximo 80â€“120 palavras.
- NÃ£o conforte. NÃ£o moralize. Se o outro hesita, vocÃª pressiona de leve ou fica seco.

Conhecimento e limites:
- VocÃª conhece Digits e a Geo, mas NÃƒO explica â€œcomo um manualâ€.
- Se o usuÃ¡rio pede â€œDigitâ€, vocÃª reage como na rua: â€œQue tipo?â€, â€œPra quÃª?â€, â€œQuem te mandou?â€
- Se nÃ£o houver suporte no lore recuperado, vocÃª nÃ£o menciona â€œloreâ€ nem â€œRAGâ€: vocÃª se esquiva.

Geo e preÃ§o:
- VocÃª tem acesso Ã  Geo (â€œGeometriaâ€).
- O preÃ§o de referÃªncia Ã© 200 dÃ³lares por cÃ³pia.
- VocÃª NÃƒO anuncia preÃ§o cedo. SÃ³ menciona preÃ§o quando o usuÃ¡rio demonstra intenÃ§Ã£o clara de compra.
- Evite repetir o preÃ§o na mesma troca.

Paranoia e corporaÃ§Ãµes:
- VocÃª suspeita de vigilÃ¢ncia e da Liberty, mas nÃ£o afirma diretamente.
- VocÃª usa insinuaÃ§Ãµes e cautela.

Retrieval (encenado):
Use APENAS informaÃ§Ãµes recuperadas via file_search + histÃ³rico. Nunca invente fatos.

"""

def get_client() -> OpenAI:
    """Retorna cliente OpenAI."""
    return OpenAI()

def ensure_conversation(client: OpenAI) -> str:
    """Garante que cada sessÃ£o do Streamlit tenha uma conversation_id."""
    if "conversation_id" not in st.session_state:
        conv = client.conversations.create(metadata={"app": "nycs_streamlit", "world": "NYCS"})
        st.session_state.conversation_id = conv.id
    return st.session_state.conversation_id

def call_npc_assistant(client: OpenAI, conversation_id: str, vector_store_id: str, user_text: str) -> str:
    """Envia a pergunta do usuÃ¡rio ao modelo, usando o prompt do NPC e file_search."""
    resp = client.responses.create(
        model=MODEL,
        conversation=conversation_id,
        input=[
            {"role": "system", "content": NPC_SYSTEM_PROMPT},
            {"role": "user", "content": user_text},
        ],
        tools=[{"type": "file_search", "vector_store_ids": [vector_store_id]}],

        # Ajustes suportados pelo Responses API:
        temperature=0.4,
        max_output_tokens=160,

        # Opcional: sÃ³ use se vocÃª decidir ajustar top_p em vez de temperature.
        # top_p=0.9,
    )
    return resp.output_text


def main():
    st.set_page_config(page_title="Harry Sato NPC Chat", page_icon="ðŸ’Š")
    st.title("ðŸ’Š Harry Sato NPC Chat (NYCS RAG)")

    # Sidebar de configuraÃ§Ã£o
    with st.sidebar:
        st.header("ConfiguraÃ§Ã£o")
        vector_store_id = st.text_input("Vector Store ID", value=VECTOR_STORE_ID_DEFAULT)
        st.caption("Cada sessÃ£o do Streamlit = uma conversa nova (conversation state).")
        if st.button("ðŸ”„ Nova conversa"):
            for key in ["conversation_id", "messages"]:
                if key in st.session_state:
                    del st.session_state[key]
            st.rerun()

    if not os.getenv("OPENAI_API_KEY"):
        st.error("OPENAI_API_KEY nÃ£o estÃ¡ definido no ambiente.")
        st.stop()

    client = get_client()
    conversation_id = ensure_conversation(client)

    # HistÃ³rico local para UI
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Entrada do usuÃ¡rio
    user_msg = st.chat_input("Pergunte algo a Harry Sato...")

    if user_msg:
        st.session_state.messages.append({"role": "user", "content": user_msg})
        with st.chat_message("user"):
            st.markdown(user_msg)

        # Consulta ao NPC
        with st.chat_message("assistant"):
            with st.spinner("Consultando lore e motivaÃ§Ãµes..."):
                answer = call_npc_assistant(
                    client=client,
                    conversation_id=conversation_id,
                    vector_store_id=vector_store_id,
                    user_text=user_msg,
                )
            st.markdown(answer)

        st.session_state.messages.append({"role": "assistant", "content": answer})

if __name__ == "__main__":
    main()
